{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6996487,"sourceType":"datasetVersion","datasetId":4021796}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications import vgg19\nfrom keras import backend as K","metadata":{"id":"EtDKm1YmwUhV","execution":{"iopub.status.busy":"2023-11-18T14:49:57.249321Z","iopub.execute_input":"2023-11-18T14:49:57.250659Z","iopub.status.idle":"2023-11-18T14:49:57.256669Z","shell.execute_reply.started":"2023-11-18T14:49:57.250619Z","shell.execute_reply":"2023-11-18T14:49:57.255277Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess images\ndef load_and_preprocess_image(image_path, target_size=(100, 100)):\n    img = load_img(image_path, target_size=target_size)\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img","metadata":{"id":"2PTiVz7kwfEl","execution":{"iopub.status.busy":"2023-11-18T14:49:57.258972Z","iopub.execute_input":"2023-11-18T14:49:57.259389Z","iopub.status.idle":"2023-11-18T14:49:57.281757Z","shell.execute_reply.started":"2023-11-18T14:49:57.259357Z","shell.execute_reply":"2023-11-18T14:49:57.280453Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Convert a tensor to an image\ndef deprocess_image(x):\n    x = x.reshape((x.shape[1], x.shape[2], 3))\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","metadata":{"id":"ixvW58LuwpLa","execution":{"iopub.status.busy":"2023-11-18T14:49:57.283195Z","iopub.execute_input":"2023-11-18T14:49:57.283594Z","iopub.status.idle":"2023-11-18T14:49:57.294099Z","shell.execute_reply.started":"2023-11-18T14:49:57.283560Z","shell.execute_reply":"2023-11-18T14:49:57.293143Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"content_layer = 'block5_conv2'\nstyle_layers = [\n    'block1_conv1',\n    'block2_conv1',\n    'block3_conv1',\n    'block4_conv1',\n    'block5_conv1',\n]","metadata":{"id":"grRajEcJy6hx","execution":{"iopub.status.busy":"2023-11-18T14:49:57.296317Z","iopub.execute_input":"2023-11-18T14:49:57.297277Z","iopub.status.idle":"2023-11-18T14:49:57.305882Z","shell.execute_reply.started":"2023-11-18T14:49:57.297234Z","shell.execute_reply":"2023-11-18T14:49:57.305000Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_model():\n  # Define the model\n    vgg = vgg19.VGG19(weights='imagenet', include_top=False)\n    vgg.trainable = False\n    outputs = [vgg.get_layer(layer).output for layer in [content_layer] + style_layers]\n    model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n    return model\n","metadata":{"id":"9rfArlJOxNgZ","execution":{"iopub.status.busy":"2023-11-18T14:49:57.307431Z","iopub.execute_input":"2023-11-18T14:49:57.308142Z","iopub.status.idle":"2023-11-18T14:49:57.320528Z","shell.execute_reply.started":"2023-11-18T14:49:57.308109Z","shell.execute_reply":"2023-11-18T14:49:57.319337Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define the content loss\ndef content_loss(base, target):\n    return tf.reduce_mean(tf.square(base - target))\n\n# Define the style loss\ndef gram_matrix(x):\n    channels = int(x.shape[-1])\n    features = K.batch_flatten(K.reshape(x, (-1, channels)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\ndef style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = tf.math.reduce_prod(tf.shape(combination))\n    return tf.reduce_mean(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n\n# Combine content and style losses\ndef total_loss(content_loss, style_loss, content_weight=1, style_weight=1e4):\n    return content_weight * content_loss + style_weight * style_loss\n\n# Define gradients and loss\ndef compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n    model_outputs = model(init_image)\n    style_output_features = model_outputs[1:]\n    content_output_features = model_outputs[:1]\n\n    style_score = 0\n    content_score = 0\n\n    weight_per_style_layer = 1.0 / float(len(style_layers))\n    for target_style, comb_style in zip(gram_style_features, style_output_features):\n        style_score += weight_per_style_layer * style_loss(comb_style[0], target_style)\n\n    weight_per_content_layer = 1.0 / float(len(content_layer))\n    content_score += weight_per_content_layer * content_loss(content_output_features[0], content_features[0])\n\n    style_score *= loss_weights[0]\n    content_score *= loss_weights[1]\n\n    loss = style_score + content_score\n    return loss, style_score, content_score","metadata":{"id":"DRxyKu_oxSB2","execution":{"iopub.status.busy":"2023-11-18T14:49:57.322195Z","iopub.execute_input":"2023-11-18T14:49:57.323368Z","iopub.status.idle":"2023-11-18T14:49:57.336929Z","shell.execute_reply.started":"2023-11-18T14:49:57.323332Z","shell.execute_reply":"2023-11-18T14:49:57.335727Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define the optimizer\ndef compute_grads(cfg):\n    with tf.GradientTape() as tape:\n        loss, _, _ = compute_loss(**cfg)\n    grads = tape.gradient(loss, cfg['init_image'])\n    return grads","metadata":{"id":"KjKlVuxHxYm_","execution":{"iopub.status.busy":"2023-11-18T14:49:57.338525Z","iopub.execute_input":"2023-11-18T14:49:57.339290Z","iopub.status.idle":"2023-11-18T14:49:57.354618Z","shell.execute_reply.started":"2023-11-18T14:49:57.339246Z","shell.execute_reply":"2023-11-18T14:49:57.353544Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Define the content and style images\ncontent_image_path = '../input/rickshaw-test-images/test.png'\nstyle_image_path = '../input/rickshaw-test-images/rick.jpg'\n\ncontent_image = load_and_preprocess_image(content_image_path)\nstyle_image = load_and_preprocess_image(style_image_path)","metadata":{"id":"QIbmaABCxmV8","execution":{"iopub.status.busy":"2023-11-18T14:49:57.355893Z","iopub.execute_input":"2023-11-18T14:49:57.356302Z","iopub.status.idle":"2023-11-18T14:49:57.414961Z","shell.execute_reply.started":"2023-11-18T14:49:57.356266Z","shell.execute_reply":"2023-11-18T14:49:57.414073Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Style transfer function\ndef style_transfer(content_image, style_image, iterations=500, content_weight=1, style_weight=1e4):\n    model = get_model()\n\n    # Compute content and style features\n    content_features = model(content_image)[:1]\n    style_features = model(style_image)[1:]\n\n    gram_style_features = [gram_matrix(style_feature[0]) for style_feature in style_features]\n\n    # Initialize the generated image with the content image\n    init_image = tf.Variable(content_image, dtype=tf.float32)\n\n    # Define the optimizer\n    opt = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n\n    # Define configuration\n    loss_weights = (style_weight, content_weight)\n    cfg = {\n        'model': model,\n        'loss_weights': loss_weights,\n        'init_image': init_image,\n        'gram_style_features': gram_style_features,\n        'content_features': content_features\n    }\n\n    # Style transfer loop\n    for i in range(iterations):\n        grads = compute_grads(cfg)\n        opt.apply_gradients([(grads, init_image)])\n        clipped = tf.clip_by_value(init_image, 0.0, 255.0)\n        init_image.assign(clipped)\n\n        if i % 100 == 0:\n            print(f\"Iteration {i}\")\n\n    return deprocess_image(init_image.numpy())","metadata":{"id":"Qmw0RUerxatf","execution":{"iopub.status.busy":"2023-11-18T14:49:57.416482Z","iopub.execute_input":"2023-11-18T14:49:57.417117Z","iopub.status.idle":"2023-11-18T14:49:57.425041Z","shell.execute_reply.started":"2023-11-18T14:49:57.417084Z","shell.execute_reply":"2023-11-18T14:49:57.423971Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Run style transfer\ntarget_size = (100, 100)  # Adjust the target size as needed\nresult = style_transfer(content_image, style_image, iterations=500, content_weight=1, style_weight=1e4)\n\n# Display the result\nimport matplotlib.pyplot as plt\nplt.imshow(result)\nplt.show()","metadata":{"id":"zTV9ZFfNxi9L","execution":{"iopub.status.busy":"2023-11-18T14:49:57.427726Z","iopub.execute_input":"2023-11-18T14:49:57.428325Z","iopub.status.idle":"2023-11-18T14:50:14.944311Z","shell.execute_reply.started":"2023-11-18T14:49:57.428292Z","shell.execute_reply":"2023-11-18T14:50:14.942553Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run style transfer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Adjust the target size as needed\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mstyle_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mstyle_transfer\u001b[0;34m(content_image, style_image, iterations, content_weight, style_weight)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Style transfer loop\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m---> 29\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     opt\u001b[38;5;241m.\u001b[39mapply_gradients([(grads, init_image)])\n\u001b[1;32m     31\u001b[0m     clipped \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(init_image, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m255.0\u001b[39m)\n","Cell \u001b[0;32mIn[8], line 4\u001b[0m, in \u001b[0;36mcompute_grads\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_grads\u001b[39m(cfg):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m----> 4\u001b[0m         loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_image\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grads\n","Cell \u001b[0;32mIn[7], line 34\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(model, loss_weights, init_image, gram_style_features, content_features)\u001b[0m\n\u001b[1;32m     32\u001b[0m weight_per_style_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(style_layers))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_style, comb_style \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(gram_style_features, style_output_features):\n\u001b[0;32m---> 34\u001b[0m     style_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight_per_style_layer \u001b[38;5;241m*\u001b[39m \u001b[43mstyle_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomb_style\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_style\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m weight_per_content_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(content_layer))\n\u001b[1;32m     37\u001b[0m content_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight_per_content_layer \u001b[38;5;241m*\u001b[39m content_loss(content_output_features[\u001b[38;5;241m0\u001b[39m], content_features[\u001b[38;5;241m0\u001b[39m])\n","Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mstyle_loss\u001b[0;34m(style, combination)\u001b[0m\n\u001b[1;32m     15\u001b[0m channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     16\u001b[0m size \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_prod(tf\u001b[38;5;241m.\u001b[39mshape(combination))\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(S \u001b[38;5;241m-\u001b[39m C)) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241;43m4.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: Cannot convert 36.0 to EagerTensor of dtype int32"],"ename":"TypeError","evalue":"Cannot convert 36.0 to EagerTensor of dtype int32","output_type":"error"}]}]}